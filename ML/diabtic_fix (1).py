# -*- coding: utf-8 -*-
"""diabtic-fix.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nzQzAiqLXy_egoU7KibXf5FIN4GJb52j

# Import Library and Load Data
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from xgboost import XGBClassifier
from sklearn.svm import SVC
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
import tensorflow as tf
from joblib import dump
import joblib

# Load data
url = "https://raw.githubusercontent.com/IrendraLintang/Diabtic/refs/heads/main/diabetes_data.csv"
df = pd.read_csv(url)

print(df.head())
df.info()

"""## Cleaning data"""

df.isnull().sum()

#drop string/object variable to check the weight
columns_to_drop = [
    'DoctorInCharge'
]
df_cleaned = df.drop(columns=columns_to_drop)
correlation_matrix = df_cleaned.corr()

# heatmap visualization
plt.figure(figsize=(12, 10))
sns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='coolwarm', square=True, cbar_kws={'shrink': .8})

plt.title('Heatmap Korelasi Antar Fitur')
plt.show()

"""## Drop unnecessary feature

The following feature are the features that has a low weight so it is considered better not to use it because it does not have a high impact
"""

columns_to_drop = [
    'PatientID',
    'Ethnicity',
    'SocioeconomicStatus',
    'EducationLevel',
    'PolycysticOvarySyndrome',
    'SerumCreatinine',
    'BUNLevels',
    'CholesterolLDL',
    'CholesterolHDL',
    'CholesterolTriglycerides',
    'Statins',
    'DoctorInCharge'
]

df_cleaned = df.drop(columns=columns_to_drop)

df_cleaned.describe().T

"""# Data Distribution

# Check the Diagnosis variable distribution
"""

X = df_cleaned.drop(columns=['Diagnosis'])
y = df_cleaned['Diagnosis']

plt.figure(figsize=(8, 5))
sns.countplot(x=y, palette="viridis")
plt.title("Distribusi Target (Diabetes vs Tidak)")
plt.xlabel("Kelas Target (0 = Tidak Diabetes, 1 = Diabetes)")
plt.ylabel("Jumlah")
plt.show()

print("Distribusi Target:")
print(y.value_counts())

"""## Data distribution through the age, BMI, gender."""

plt.figure(figsize=(8, 6))
sns.countplot(data=df_cleaned, x='Gender', palette='Set2')
plt.title('Distribution of Diabetes Diagnoses by Gender', fontsize=16)
plt.xlabel('Gender', fontsize=14)
plt.ylabel('Count', fontsize=14)
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.show()

plt.figure(figsize=(12, 6))
sns.histplot(data=df_cleaned, x='Age', bins=20, kde=False, color='blue')
plt.title('Distribution of Diabetes Diagnoses by Age', fontsize=16)
plt.xlabel('Age', fontsize=14)
plt.ylabel('Count', fontsize=14)
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.show()

plt.figure(figsize=(12, 6))
sns.histplot(data=df_cleaned, x='BMI', bins=20, kde=True, color='green')
plt.title('Distribution of Diabetes Diagnoses by BMI', fontsize=16)
plt.xlabel('BMI', fontsize=14)
plt.ylabel('Count', fontsize=14)
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.show()

plt.figure(figsize=(12, 6))
sns.histplot(data=df_cleaned, x='FastingBloodSugar', bins=20, kde=True, color='yellow')
plt.title('Distribution of Diabetes Diagnoses by Blood Sugar', fontsize=16)
plt.xlabel('Fasting Blood Sugar', fontsize=14)
plt.ylabel('Count', fontsize=14)
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.show()

"""# Visualization of Correlation Between Features"""

# correlation
correlation_matrix = df_cleaned.corr()

# heatmap Visualization
plt.figure(figsize=(12, 10))
sns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='coolwarm', square=True, cbar_kws={'shrink': .8})

plt.title('Heatmap Korelasi Antar Fitur')
plt.show()

selected_features = [
    'Age',                # Usia
    'BMI',                # Indeks Massa Tubuh
    'FastingBloodSugar',  # Gula darah puasa
    'DietQuality',
    'SystolicBP',         # Tekanan darah sistolik
    'DiastolicBP',        # Tekanan darah diastolik
    'FatigueLevels',      # Tingkat kelelahan
    'PhysicalActivity',
    'HbA1c',
    'Diagnosis'           # Diagnosis (target)
]
sns.pairplot(df_cleaned[selected_features], hue='Diagnosis', palette='Set2', diag_kind='kde')
plt.show()

"""# Building Model"""

X = df_cleaned.drop(columns=['Diagnosis'])
y = df_cleaned['Diagnosis']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

"""# DNN"""

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, Input

model = Sequential([
    Input(shape=(X_train_scaled.shape[1],)),
    Dense(64, activation='relu'),
    Dropout(0.3),
    Dense(32, activation='relu'),
    Dense(1, activation='sigmoid')
])

model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

history = model.fit(X_train_scaled, y_train, epochs=50, batch_size=32, validation_data=(X_test_scaled, y_test))

# Model evaluation on test data
loss, accuracy = model.evaluate(X_test_scaled, y_test)
print(f"Akurasi pada data uji: {accuracy:.4f}")

# Manual prediction and accuracy
y_pred = (model.predict(X_test_scaled) > 0.5).astype("int32")
manual_accuracy = accuracy_score(y_test, y_pred)
print(f"Akurasi manual: {manual_accuracy:.4f}")

# Accuracy and Loss graph during training
plt.figure(figsize=(12, 5))

# Accuration
plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Training and Validation Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()

# Loss
plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Training and Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()

plt.tight_layout()
plt.show()

"""# Random Forest"""

from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score

# Random Forest
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(X_train_scaled, y_train)
y_pred_rf = rf_model.predict(X_test_scaled)
print("Akurasi Random Forest:", accuracy_score(y_test, y_pred_rf))
print("Laporan Klasifikasi:\n", classification_report(y_test, y_pred_rf))

"""# XGBoost"""

# XGBoost
xgb_model = XGBClassifier(eval_metric='logloss', verbosity=0)
xgb_model.fit(X_train_scaled, y_train)
y_pred_xgb = xgb_model.predict(X_test_scaled)
print("Akurasi XGBoost:", accuracy_score(y_test, y_pred_xgb))
print("Laporan Klasifikasi:\n", classification_report(y_test, y_pred_xgb))

"""# SVM"""

# SVM
svm_model = SVC(kernel='rbf', probability=True)
svm_model.fit(X_train_scaled, y_train)
y_pred_svm = svm_model.predict(X_test_scaled)
print("Akurasi SVM:", accuracy_score(y_test, y_pred_svm))
print("Laporan Klasifikasi:\n", classification_report(y_test, y_pred_svm))

"""# Ensemble Learning (Soft Voting)

Combining the prediction probabilities from all models and taking the average as the final decision.
"""

# Probability of each model
rf_probs = rf_model.predict_proba(X_test_scaled)[:, 1]
xgb_probs = xgb_model.predict_proba(X_test_scaled)[:, 1]
svm_probs = svm_model.predict_proba(X_test_scaled)[:, 1]
dnn_probs = model.predict(X_test_scaled).flatten()

# Soft voting
combined_probs = (rf_probs + xgb_probs + svm_probs + dnn_probs) / 4
final_predictions = (combined_probs > 0.5).astype(int)

# Evaluation of ensemble results
print("Akurasi Ensemble:", accuracy_score(y_test, final_predictions))
print("Laporan Klasifikasi:\n", classification_report(y_test, final_predictions))

conf_matrix = confusion_matrix(y_test, final_predictions)
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')
plt.xlabel('Prediksi')
plt.ylabel('Aktual')
plt.title('Confusion Matrix Ensemble Model')
plt.show()

# Random Forest
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(X_train_scaled, y_train)
dump(rf_model, 'rf_model.pkl')

# XGBoost
xgb_model = XGBClassifier(eval_metric='logloss', verbosity=0)
xgb_model.fit(X_train_scaled, y_train)
dump(xgb_model, 'xgb_model.pkl')

# SVM
svm_model = SVC(kernel='rbf', probability=True)
svm_model.fit(X_train_scaled, y_train)
dump(svm_model, 'svm_model.pkl')

# DNN
model.save('dnn_model.h5')

# Load individual models
rf_model = joblib.load('rf_model.pkl')
xgb_model = joblib.load('xgb_model.pkl')
svm_model = joblib.load('svm_model.pkl')
dnn_model = tf.keras.models.load_model('dnn_model.h5')

# Create an external prediction function
def external_models_prediction(inputs):
    rf_probs = rf_model.predict_proba(inputs)[:, 1]
    xgb_probs = xgb_model.predict_proba(inputs)[:, 1]
    svm_probs = svm_model.predict_proba(inputs)[:, 1]
    return np.column_stack((rf_probs, xgb_probs, svm_probs)).astype(np.float32)

# Ensemble Layer for DNN
class EnsembleLayer(tf.keras.layers.Layer):
    def __init__(self, dnn_model, **kwargs):
        super(EnsembleLayer, self).__init__(**kwargs)
        self.dnn_model = dnn_model

    def call(self, inputs):
        non_dnn_probs = tf.numpy_function(external_models_prediction, [inputs], tf.float32)
        dnn_probs = tf.squeeze(self.dnn_model(inputs))
        combined_probs = (non_dnn_probs[:, 0] + non_dnn_probs[:, 1] + non_dnn_probs[:, 2] + dnn_probs) / 4
        return tf.expand_dims(tf.cast(combined_probs > 0.5, tf.int32), axis=-1)

    def compute_output_shape(self, input_shape):
        return (input_shape[0], 1)

    def get_config(self):
        config = super().get_config()
        config.update({"dnn_model": self.dnn_model})
        return config

# Create Keras ensemble models
ensemble_input = tf.keras.Input(shape=(X_train_scaled.shape[1],))
ensemble_output = EnsembleLayer(dnn_model)(ensemble_input)
ensemble_model = tf.keras.Model(inputs=ensemble_input, outputs=ensemble_output)

# Save model as HDF5 (.h5)
ensemble_model.save('ensemble_model.h5')